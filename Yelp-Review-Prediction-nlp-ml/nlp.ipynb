{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Unstructured data makes up the vast majority of data.  This is a basic intro to handling unstructured data.  My objective is to be able to extract the\n",
    "sentiment (positive or negative) from Yelp review text, and perform prediction on Yelp review ratings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill\n",
    "import ujson\n",
    "import gzip\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn import linear_model\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Sample\n",
    "test_json = [\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"WsGQfLLy3YlP_S9jBE3j1w\", \"review_id\": \"kzFlI35hkmYA_vPSsMcNoQ\", \"stars\": 5, \"date\": \"2012-11-03\", \"text\": \"Love it!!!!! Love it!!!!!! love it!!!!!!!   Who doesn't love Culver's!\", \"type\": \"review\", \"business_id\": \"LRKJF43s9-3jG9Lgx4zODg\"},\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"Veue6umxTpA3o1eEydowZg\", \"review_id\": \"Tfn4EfjyWInS-4ZtGAFNNw\", \"stars\": 3, \"date\": \"2013-12-30\", \"text\": \"Everything was great except for the burgers they are greasy and very charred compared to other stores.\", \"type\": \"review\", \"business_id\": \"LRKJF43s9-3jG9Lgx4zODg\"},\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"u5xcw6LCnnMhddoxkRIgUA\", \"review_id\": \"ZYaS2P5EmK9DANxGTV48Tw\", \"stars\": 5, \"date\": \"2010-12-04\", \"text\": \"I really like both Chinese restaurants in town.  This one has outstanding crab rangoon.  Love the chicken with snow peas and mushrooms and General Tso Chicken.  Food is always ready in 10 minutes which is accurate.  Good place and they give you free pop.\", \"type\": \"review\", \"business_id\": \"RgDg-k9S5YD_BaxMckifkg\"},\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"kj18hvJRPLepZPNL7ySKpg\", \"review_id\": \"uOLM0vvnFdp468ofLnszTA\", \"stars\": 3, \"date\": \"2011-06-02\", \"text\": \"Above average takeout with friendly staff. The sauce on the pan fried noodle is tasty. Dumplings are quite good.\", \"type\": \"review\", \"business_id\": \"RgDg-k9S5YD_BaxMckifkg\"},\n",
    "    {\"votes\": {\"funny\": 0, \"useful\": 0, \"cool\": 0}, \"user_id\": \"L5kqM35IZggaPTpQJqcgwg\", \"review_id\": \"b3u1RHmZTNRc0thlFmj2oQ\", \"stars\": 4, \"date\": \"2012-05-28\", \"text\": \"We order from Chang Jiang often and have never been disappointed.  The menu is huge, and can accomodate anyone's taste buds.  The service is quick, usually ready in 10 minutes.\", \"type\": \"review\", \"business_id\": \"RgDg-k9S5YD_BaxMckifkg\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open('yelp_train_academic_dataset_review.json.gz', 'rb') as f:\n",
    "    file_content = f.read()\n",
    "b = file_content.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "for record in b:\n",
    "    if ujson.loads('['+record+']'):\n",
    "        item = ujson.loads('['+record+']')[0]\n",
    "        text = item['text']\n",
    "        stars = item['stars']\n",
    "        business_id = item['business_id']\n",
    "        dic = {\"stars\":stars, \"text\":text}\n",
    "        dict_list.append(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## bag_of_words_model\n",
    "Build a linear model based on the count of the words in each document\n",
    "(bag-of-words model).\n",
    "\n",
    "**Hints**:\n",
    "1. Don't forget to use tokenization!  This is important for good performance\n",
    "   but it is also the most expensive step.  Try vectorizing as a first initial\n",
    "   step:\n",
    "   ```Python\n",
    "       X = feature_extraction.text \\\n",
    "                             .CountVectorizer() \\\n",
    "                             .fit_transform(text)\n",
    "       y = scores\n",
    "   ``` \n",
    "   and then running grid-serach and cross-validation only on of this\n",
    "   pre-processed data.  `CountVectorizer` has to memorize the mapping between\n",
    "   words and the index to which it is assigned.  This is linear in the size of\n",
    "   the vocabulary.  The `HashingVectorizer` does not have to remember this\n",
    "   mapping and will lead to much smaller models.\n",
    "\n",
    "2. Try choosing different values for `min_df` (minimum document frequency\n",
    "   cutoff) and `max_df` in `CountVectorizer`.  Setting `min_df` to zero admits\n",
    "   rare words which might only appear once in the entire corpus.  This is both\n",
    "   prone to overfitting and makes your data unmanageably large.  Don't forget\n",
    "   to use cross-validation or to select the right value.  Notice that\n",
    "   `HashingVectorizer` doesn't support `min_df`  and `max_df`.  However, it's\n",
    "   not hard to roll your own transformer that solves for these.\n",
    "\n",
    "3. Try using `LinearRegression` or `RidgeCV`.  If the memory footprint is too\n",
    "   big, try switching to Stochastic Gradient Descent\n",
    "   (`sklearn.linear_model.SGDRegressor`) You might find that even ordinary\n",
    "   linear regression fails due to the data size.  Don't forget to use\n",
    "   `GridSearchCV` to determine the regularization parameter!  How do the\n",
    "   regularization parameter `alpha` and the values of `min_df` and `max_df`\n",
    "   from `CountVectorizer` change the answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a text list\n",
    "text_list = []\n",
    "for item in dict_list:\n",
    "    text_list.append(item['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a star list \n",
    "Y = []\n",
    "for item in dict_list:\n",
    "    Y.append(item['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build an estimator \n",
    "class q1estimator(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self):\n",
    "        self.clf = linear_model.SGDRegressor()\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        self.count_vect = HashingVectorizer()\n",
    "        X_train = self.count_vect.transform(text_list)\n",
    "        self.clf.fit(X_train, Y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, record):\n",
    "        transformed_feature = self.count_vect.transform([record[\"text\"]])\n",
    "        value = self.clf.predict(transformed_feature)[0]\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SGDRegressor = q1estimator()\n",
    "q1_estimator = SGDRegressor.fit()\n",
    "\n",
    "dill.dump(q1_estimator, open(\"NLPQ1\",\"w\")) \n",
    "predicted_value = q1_estimator.predict(test_json[1])\n",
    "print predicted_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalized_model\n",
    "Normalization is key for good linear regression. Previously, we used the count\n",
    "as the normalization scheme.  Try some of these alternative vectorizations:\n",
    "\n",
    "1. You can use the \"does this word present in this document\" as a normalization\n",
    "   scheme, which means the values are always 1 or 0.  So we give no additional\n",
    "   weight to the presence of the word multiple times.\n",
    "\n",
    "2. Try using the log of the number of counts (or more precisely, $log(x+1)$).\n",
    "   This is often used because we want the repeated presence of a word to count\n",
    "   for more but not have that effect tapper off.\n",
    "\n",
    "3. [TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a common\n",
    "   normalization scheme used in text processing.  Use the `TFIDFTransformer`.\n",
    "   There are options for using `idf` and taking the logarithm of `tf`.  Do\n",
    "   these significantly affect the result?\n",
    "\n",
    "Finally, if you can't decide which one is better, don't forget that you can\n",
    "combine models with a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Question 2 Normalized Model\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "class q2estimator(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self):\n",
    "        self.clf = linear_model.SGDRegressor()\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        self.count_vect = HashingVectorizer()\n",
    "        X_train = self.count_vect.transform(text_list)\n",
    "        self.tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "        X_train_tf = self.tf_transformer.transform(X_train)\n",
    "        self.clf.fit(X_train_tf, Y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, record):\n",
    "        transformed_feature = self.count_vect.transform([record[\"text\"]])\n",
    "        X_train_tf = self.tf_transformer.transform(transformed_feature)\n",
    "        value = self.clf.predict(X_train_tf)[0]\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q2Regressor = q2estimator()\n",
    "q2_estimator = q2Regressor.fit()\n",
    "dill.dump(q2_estimator, open(\"NLPQ2\",\"w\")) \n",
    "q2_estimator.predict(test_json[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## bigram_model\n",
    "In a bigram model, let's consider both single words and pairs of consecutive\n",
    "words that appear.  This is going to be a much higher dimensional problem\n",
    "(large $p$) so you should be careful about overfitting.\n",
    "\n",
    "Sometimes, reducing the dimension can be useful.  Because we are dealing with a\n",
    "sparse matrix, we have to use `TruncatedSVD`.  If we reduce the dimensions, we\n",
    "can use a more sophisticated models than linear ones.\n",
    "\n",
    "As before, memory problems can crop up due to the engineering constraints.\n",
    "Playing with the number of features, using the `HashingVectorizer`,\n",
    "incorporating `min_df` and `max_df` limits, and handling stop-words in some way\n",
    "are all methods of addressing this issue. If you are using `CountVectorizer`,\n",
    "it is possible to run it with a fixed vocabulary (based on a training run, for\n",
    "instance). Check the documentation.\n",
    "\n",
    "*** A side note on multi-stage model evaluation: When your model consists of a\n",
    "pipeline with several stages, it can be worthwhile to evaluate which parts of\n",
    "the pipeline have the greatest impact on the overall accuracy (or other metric)\n",
    "of the model. This allows you to focus your efforts on improving the important\n",
    "algorithms, and leaving the rest \"good enough\".\n",
    "\n",
    "One way to accomplish this is through ceiling analysis, which can be useful\n",
    "when you have a training set with ground truth values at each stage. Let's say\n",
    "you're training a model to extract image captions from websites and return a\n",
    "list of names that were in the caption. Your overall accuracy at some point\n",
    "reaches 70%. You can try manually giving the model what you know are the\n",
    "correct image captions from the training set, and see how the accuracy improves\n",
    "(maybe up to 75%). Alternatively, giving the model the perfect name parsing for\n",
    "each caption increases accuracy to 90%. This indicates that the name parsing is\n",
    "a much more promising target for further work, and the caption extraction is a\n",
    "relatively smaller factor in the overall performance.\n",
    "\n",
    "If you don't know the right answers at different stages of the pipeline, you\n",
    "can still evaluate how important different parts of the model are to its\n",
    "performance by changing or removing certain steps while keeping everything\n",
    "else constant. You might try this kind of analysis to determine how important\n",
    "adding stopwords and stemming to your NLP model actually is, and how that\n",
    "importance changes with parameters like the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Question 3 Bigram Model\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "bigram_pipeline = Pipeline([\n",
    "                            ('vectorize',HashingVectorizer(n_features = 100000, ngram_range=(1,2),stop_words = \"english\")),\n",
    "                            ('dimensionality_reduce',TruncatedSVD(n_components = 10, algorithm = 'arpack',n_iter=2)),\n",
    "                            ('estimator',linear_model.SGDRegressor()),\n",
    "                        ])\n",
    "bigram_pipeline.fit(text_list,Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dill.dump(bigram_pipeline, open(\"NLPQ3\",\"w\")) \n",
    "value = bigram_pipeline.predict([test_json[4]['text']])   # test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## food_bigrams\n",
    "Look over all reviews of restaurants (you may need to look at the dataset from\n",
    "`ml.py` to figure out which ones correspond to restaurants). We want to find\n",
    "collocations --- that is, bigrams that are \"special\" and appear more often than\n",
    "you'd expect from chance.  We can think of the corpus as defining an empirical\n",
    "distribution over all ngrams.  We can find word pairs that are unlikely to\n",
    "occur consecutively based on the underlying probability of their words.\n",
    "Mathematically, if $p(w)$ be the probability of a word $w$ and $p(w_1 w_2)$ is\n",
    "the probability of the bigram $w_1 w_2$, then we want to look at word pairs\n",
    "$w_1 w_2$ where the statistic\n",
    "\n",
    "  $$ p(w_1 w_2) / (p(w_1) * p(w_2)) $$\n",
    "\n",
    "is high.  Return the top 100 (mostly food) bigrams with this statistic with\n",
    "the 'right' prior factor (see below).\n",
    "\n",
    "*Questions:* (to think about: they are not a part of the answer).  This\n",
    "statistic is a ratio and problematic when the denominator is small.  We can fix\n",
    "this by applying Bayesian smoothing to $p(w)$ (i.e. mixing the empirical\n",
    "distribution with the uniform distribution over the vocabulary).\n",
    "\n",
    "1. How does changing this smoothing parameter effect the word pairs you get\n",
    "   qualitatively?\n",
    "\n",
    "2. We can interpret the smoothing parameter as adding a constant number of\n",
    "   occurences of each word to our distribution.  Does this help you determine\n",
    "   set a reasonable value for this 'prior factor'?\n",
    "\n",
    "3. For fun: also check out [Amazon's Statistically Improbable\n",
    "   Phrases](http://en.wikipedia.org/wiki/Statistically_Improbable_Phrases).\n",
    "\n",
    "*Implementation notes:*\n",
    "- The reference solution is not an aggressive filterer. Although there are\n",
    "  definitely artifacts in the bigrams you'll find, many of the seeming nonsense\n",
    "  words are actually somewhat meaningful and so using smoothing parameters in\n",
    "  the thousands or a high min_df might give you different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# laod the data from the ml.py\n",
    "with gzip.open('yelp_train_academic_dataset_business.json.gz', 'rb') as f:\n",
    "    file_content_ml = f.read()\n",
    "a_ml = ujson.dumps(file_content_ml,encode_html_chars=True)\n",
    "b_ml = ujson.loads(a_ml)\n",
    "b_ml = b_ml.replace('\\n',',').replace('&','and')\n",
    "List = '['+b_ml[:-1]+']'\n",
    "Data_ml = ujson.loads(List)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_list_new = []\n",
    "for record in b:\n",
    "    if ujson.loads('['+record+']'):\n",
    "        item = ujson.loads('['+record+']')[0]\n",
    "        text = item['text']\n",
    "        stars = item['stars']\n",
    "        business_id = item['business_id']\n",
    "        dic = {\"text\":text,\"business_id\":business_id}\n",
    "        dict_list_new.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_review = pd.DataFrame(dict_list_new)\n",
    "df_business = pd.DataFrame(Data_ml)\n",
    "new_list = []\n",
    "for item in Data_ml:\n",
    "    if 'Restaurants' in item['categories']:\n",
    "        new_list.append(item)\n",
    "    else: continue\n",
    "df_Restaurants = pd.DataFrame(new_list)\n",
    "df_restaurants_crop = df_Restaurants[['categories','stars','business_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df_restaurants_crop, df_review, how = 'inner', on = 'business_id')\n",
    "new_text_list = df_merge['text'].values.tolist()\n",
    "Y_list = df_merge['stars'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create Unigram Model convert a collection of text documents to a matrix of token counts\n",
    "q4_uni_gram = CountVectorizer(ngram_range=(1,1),stop_words=\"english\",min_df=0.00001,max_features = 100000,strip_accents = \"ascii\")\n",
    "X_train_q4_unigram = q4_uni_gram.fit_transform(new_text_list)\n",
    "\n",
    "# Create Bigram Model\n",
    "q4_bi_gram = CountVectorizer(ngram_range=(2,2),stop_words=\"english\", min_df=0.00001,max_features = 100000,strip_accents = \"ascii\")\n",
    "X_train_q4_bigram = q4_bi_gram.fit_transform(new_text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bicount_list = X_train_q4_bigram.sum(axis=0).tolist()[0]\n",
    "total_count_biword = X_train_q4_bigram.sum()\n",
    "total_count_uniword = X_train_q4_unigram.sum()\n",
    "unicount_list = X_train_q4_unigram.sum(axis = 0).tolist()[0]\n",
    "uniword_list = sorted(q4_uni_gram.vocabulary_, key=q4_uni_gram.vocabulary_.get)\n",
    "biword_list = sorted(q4_bi_gram.vocabulary_, key=q4_bi_gram.vocabulary_.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a dictionary for biwords and uniword\n",
    "l=[]\n",
    "biword_dict = dict(zip(biword_list,bicount_list))\n",
    "uniword_dict = dict(zip(uniword_list,unicount_list))\n",
    "for word_pair, counts in biword_dict.items():\n",
    "    w1 = word_pair.split(' ')[0]\n",
    "    w2 = word_pair.split(' ')[1]\n",
    "    item = (w1,w2 ,uniword_dict[w1],uniword_dict[w2],counts)\n",
    "    l.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ratio = pd.DataFrame(l)\n",
    "df_ratio.columns = ['w1','w2','count1','count2','counts']\n",
    "df_ratio['ratio'] = (df_ratio.counts)/((df_ratio.count1)*(df_ratio.count2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a List of Biwords\n",
    "result = df_ratio.sort(['ratio'], ascending=[0])\n",
    "q4_result = result[['w1','w2']].values.tolist()\n",
    "final_l = []\n",
    "for item in q4_result:\n",
    "    new_item = item[0]+\" \"+item[1]\n",
    "    final_l.append(new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove digits in the string \n",
    "RE_D = re.compile('\\d|\\_')\n",
    "final_l_str = []\n",
    "for item in final_l:\n",
    "    if RE_D.search(item):\n",
    "        continue\n",
    "    else: \n",
    "        final_l_str.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_l_str[0:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
